# 一、使用OpenAI API

GPT有封装好的接口，我们简单封装即可。目前GPT embedding mode有三种，性能如下所示：


| 模型                   | 每美元页数 | MTEB得分 | MIRACL得分 |
| ---------------------- | ---------- | -------- | ---------- |
| text-embedding-3-large | 9,615      | 54.9     | 64.6       |
| text-embedding-3-small | 62,500     | 62.3     | 44.0       |
| text-embedding-ada-002 | 12,500     | 61.0     | 31.4       |

- MTEB得分为embedding model分类、聚类、配对等八个任务的平均得分。
- MIRACL得分为embedding model在检索任务上的平均得分。

从以上三个embedding model我们可以看出`text-embedding-3-large`有最好的性能和最贵的价格，当我们搭建的应用需要更好的表现且成本充足的情况下可以使用；`text-embedding-3-small`有着较好的性能跟价格，当我们预算有限时可以选择该模型；而`text-embedding-ada-002`是OpenAI上一代的模型，无论在性能还是价格都不如及前两者，因此不推荐使用。

```python
import os
from openai import OpenAI
from dotenv import load_dotenv, find_dotenv


# 读取本地/项目的环境变量。
# find_dotenv()寻找并定位.env文件的路径
# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  
# 如果你设置的是全局的环境变量，这行代码则没有任何作用。
_ = load_dotenv(find_dotenv())

# 如果你需要通过代理端口访问，你需要如下配置
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'
os.environ["HTTP_PROXY"] = 'http://127.0.0.1:7890'

def openai_embedding(text: str, model: str=None):
    # 获取环境变量 OPENAI_API_KEY
    api_key=os.environ['OPENAI_API_KEY']
    client = OpenAI(api_key=api_key)

    # embedding model：'text-embedding-3-small', 'text-embedding-3-large', 'text-embedding-ada-002'
    if model == None:
        model="text-embedding-3-small"

    response = client.embeddings.create(
        input=text,
        model=model
    )
    return response

response = openai_embedding(text='要生成 embedding 的输入文本，字符串形式。')
```

API返回的数据为`json`格式，除`object`向量类型外还有存放数据的`data`、embedding model 型号`model`以及本次 token 使用情况`usage`等数据，具体如下所示：

```json
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "index": 0,
      "embedding": [
        -0.006929283495992422,
        ... (省略)
        -4.547132266452536e-05,
      ],
    }
  ],
  "model": "text-embedding-3-small",
  "usage": {
    "prompt_tokens": 5,
    "total_tokens": 5
  }
}
```

我们可以调用response的object来获取embedding的类型。

```python
print(f'返回的embedding类型为：{response.object}')
```

```markup
返回的embedding类型为：list
```

embedding存放在data中，我们可以查看embedding的长度及生成的embedding。

```python
print(f'embedding长度为：{len(response.data[0].embedding)}')
print(f'embedding（前10）为：{response.data[0].embedding[:10]}')
```

```markup
embedding长度为：1536
embedding（前10）为：[0.03884002938866615, 0.013516489416360855, -0.0024250170681625605, -0.01655769906938076, 0.024130908772349358, -0.017382603138685226, 0.04206013306975365, 0.011498954147100449, -0.028245486319065094, -0.00674333656206727]
```

我们也可以查看此次embedding的模型及token使用情况。

```python
print(f'本次embedding model为：{response.model}')
print(f'本次token使用情况为：{response.usage}')
```

```markup
本次embedding model为：text-embedding-3-small
本次token使用情况为：Usage(prompt_tokens=12, total_tokens=12)
```

