# 一、词向量

## 1.1 什么是词向量

![Embeddding](https://datawhalechina.github.io/llm-universe/figures/C3-1-embedding.png)

在机器学习和自然语言处理（NLP）中，词向量（Embeddings）是一种将非结构化数据，如单词、句子或者整个文档，转化为实数向量的技术。这些实数向量可以被计算机更好地理解和处理。

嵌入背后的主要想法是，**相似或相关的对象在嵌入空间中的距离应该很近。**

![similar](https://datawhalechina.github.io/llm-universe/figures/C3-2-similar.png)

举个例子，我们可以使用词嵌入（word embeddings）来表示文本数据。在词嵌入中，每个单词被转换为一个向量，这个向量捕获了这个单词的语义信息。例如，"king" 和 "queen" 这两个单词在嵌入空间中的位置将会非常接近，因为它们的含义相似。而 "apple" 和 "orange" 也会很接近，因为它们都是水果。而 "king" 和 "apple" 这两个单词在嵌入空间中的距离就会比较远，因为它们的含义不同。

## 1.2 词向量的优势

在RAG（Retrieval Augmented Generation，检索增强生成）方面词向量的优势主要有两点：

- 词向量比文字更适合检索。当我们在数据库检索时，如果数据库存储的是文字，主要通过检索关键词（词法搜索）等方法找到相对匹配的数据，<u>匹配的程度是取决于关键词的数量或者是否完全匹配查询句的</u>；<u>但是词向量中包含了原文本的语义信息，可以通过计算问题与数据库中数据的点积、余弦距离、欧几里得距离等指标，直接获取问题与数据在语义层面上的相似度；</u>
- 词向量比其它媒介的综合信息能力更强，当传统数据库存储文字、声音、图像、视频等多种媒介时，很难去将上述多种媒介构建起关联与跨模态的查询方法；**但是词向量却可以通过多种向量模型将多种数据映射成统一的向量形式。**

## 1.3 一般构建词向量的方法

在搭建 RAG 系统时，我们往往可以通过使用嵌入模型来构建词向量，我们可以选择：

- 使用各个公司的 Embedding API；
- 在本地使用嵌入模型将数据构建为词向量。

# 二、向量数据库

## 2.1 什么是向量数据库

向量数据库是用于高效计算和管理大量向量数据的解决方案。向量数据库是一种专门用于存储和检索向量数据（embedding）的数据库系统。它与传统的基于关系模型的数据库不同，它主要关注的是向量数据的特性和相似性。

在向量数据库中，数据被表示为向量形式，每个向量代表一个数据项。这些向量可以是数字、文本、图像或其他类型的数据。向量数据库使用高效的索引和查询算法来加速向量数据的存储和检索过程。

## 2.2 向量数据库的原理及核心优势

向量数据库中的数据以向量作为基本单位，对向量进行存储、处理及检索。向量数据库通过计算与目标向量的余弦距离、点积等获取与目标向量的相似度。当处理大量甚至海量的向量数据时，向量数据库索引和查询算法的效率明显高于传统数据库。

## 2.3 主流的向量数据库

- [Chroma](https://www.trychroma.com/)：是一个轻量级向量数据库，拥有丰富的功能和简单的 API，具有简单、易用、轻量的优点，但功能相对简单且不支持GPU加速，适合初学者使用。
- [Weaviate](https://weaviate.io/)：是一个开源向量数据库。除了支持相似度搜索和最大边际相关性（MMR，Maximal Marginal Relevance）搜索外还可以支持结合多种搜索算法（基于词法搜索、向量搜索）的混合搜索，从而搜索提高结果的相关性和准确性。
- [Qdrant](https://qdrant.tech/)：Qdrant使用 Rust 语言开发，有极高的检索效率和RPS（Requests Per Second），支持本地运行、部署在本地服务器及Qdrant云三种部署模式。且可以通过为页面内容和元数据制定不同的键来复用数据。

# 三、通过向量数据库检索

## 3.1 相似度检索

```python
question="什么是机器学习"
```

```python
sim_docs = vectordb.similarity_search(question,k=3)
print(f"检索到的内容数：{len(sim_docs)}")
```

```markup
检索到的内容数：3
```

```python
for i, sim_doc in enumerate(sim_docs):
    print(f"检索到的第{i}个内容: \n{sim_doc.page_content[:200]}", end="\n--------------\n")
```

```markup
检索到的第0个内容: 
导，同时也能体会到这三门数学课在机器学习上碰撞产生的“数学之美”。
1.1
引言
本节以概念理解为主，在此对“算法”和“模型”作补充说明。“算法”是指从数据中学得“模型”的具
体方法，例如后续章节中将会讲述的线性回归、对数几率回归、决策树等。“算法”产出的结果称为“模型”，
通常是具体的函数或者可抽象地看作为函数，例如一元线性回归算法产出的模型即为形如 f(x) = wx + b
的一元一次函数。
--------------
检索到的第1个内容: 
模型：机器学习的一般流程如下：首先收集若干样本（假设此时有 100 个），然后将其分为训练样本
（80 个）和测试样本（20 个），其中 80 个训练样本构成的集合称为“训练集”，20 个测试样本构成的集合
称为“测试集”，接着选用某个机器学习算法，让其在训练集上进行“学习”（或称为“训练”），然后产出
得到“模型”（或称为“学习器”），最后用测试集来测试模型的效果。执行以上流程时，表示我们已经默
--------------
检索到的第2个内容: 
→_→
欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》
←_←
第 1 章
绪论
本章作为“西瓜书”的开篇，主要讲解什么是机器学习以及机器学习的相关数学符号，为后续内容作
铺垫，并未涉及复杂的算法理论，因此阅读本章时只需耐心梳理清楚所有概念和数学符号即可。此外，在
阅读本章前建议先阅读西瓜书目录前页的《主要符号表》，它能解答在阅读“西瓜书”过程中产生的大部
分对数学符号的疑惑。
本章也作为
--------------
```

## 3.2 MMR 检索

如果只考虑检索出内容的**相关性**会导致内容过于单一，可能丢失重要信息。

最大边际相关性 (`MMR, Maximum marginal relevance`) 可以帮助我们在保持相关性的同时，增加内容的丰富度。

**核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。**这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。

```python
mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)
```

```python
for i, sim_doc in enumerate(mmr_docs):
    print(f"MMR 检索到的第{i}个内容: \n{sim_doc.page_content[:200]}", end="\n--------------\n")
```

```markup
MMR 检索到的第0个内容: 
导，同时也能体会到这三门数学课在机器学习上碰撞产生的“数学之美”。
1.1
引言
本节以概念理解为主，在此对“算法”和“模型”作补充说明。“算法”是指从数据中学得“模型”的具
体方法，例如后续章节中将会讲述的线性回归、对数几率回归、决策树等。“算法”产出的结果称为“模型”，
通常是具体的函数或者可抽象地看作为函数，例如一元线性回归算法产出的模型即为形如 f(x) = wx + b
的一元一次函数。
--------------
MMR 检索到的第1个内容: 
而人工智能的基本挑战是

学习在不确定的情况下做出好的决策

这边我举个例子

比如你想让一个小孩学会走路

他就需要通过不断尝试来发现

怎么走比较好

怎么走比较快

强化学习的交互过程可以通过这张图来表示

强化学习由智能体和环境两部分组成

在强化学习过程中

智能体与环境一直在交互

智能体在环境中获取某个状态后

它会利用刚刚的状态输出一个动作

这个动作也被称为决策

然后这个动作会
--------------
MMR 检索到的第2个内容: 
与基础语言模型不同，指令微调 LLM 通过专门的训练，可以更好地理解并遵循指令。举个例子，当询问“法国的首都是什么？”时，这类模型很可能直接回答“法国的首都是巴黎”。指令微调 LLM 的训练通常基于预训练语言模型，先在大规模文本数据上进行预训练，掌握语言的基本规律。在此基础上进行进一步的训练与微调（finetune），输入是指令，输出是对这些指令的正确回复。有时还会采用RLHF（reinforce
--------------
```

可以看到内容有了更多的差异。

# 四、构造检索式问答连

我们已经可以通过向量数据库找到最相关的内容了，接下来我们可以让 LLM 来用这些相关的内容回答我们的问题。

## 4.1 直接询问 LLM

基于 LangChain，我们可以构造一个使用 LLM 进行问答的检索式问答链，这是一种通过检索步骤进行问答的方法。我们可以通过传入一个语言模型和一个向量数据库来创建它作为检索器。然后，我们可以用问题作为查询调用它，得到一个答案。

```python
# 导入检索式问答链
from langchain.chains import RetrievalQA
```

```python
llm = OpenAI(temperature=0)
```

```python
# 可以使用 HuggingFacePipeline 本地搭建大语言模型
model_id = 'THUDM/chatglm2-6b-int4' # 采用 int 量化后的模型可以节省硬盘占用以及实时量化所需的运算资源
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModel.from_pretrained(model_id, trust_remote_code=True).half().quantize(4).cuda()
model = model.eval()
pipe = pipeline(
    "text2text-generation",
    model=model, 
    tokenizer=tokenizer, 
    max_length=100
)

llm = HuggingFacePipeline(pipeline=pipe)
```

```python
llm = ZhipuAILLM(model="chatglm_std", temperature=0)
```

```python
# 声明一个检索式问答链
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever()
)
```

```python
# 可以以该方式进行检索问答
question = "本知识库主要包含什么内容"
result = qa_chain({"query": question})
print(f"大语言模型的回答为：{result['result']}")
```

```markup
大语言模型的回答为： 这个知识库主要包含了一些关于强化学习的基础知识，传统的强化学习算法，适用强化学习算法的解决方法，简单生动的例子，专业的公式推导和分析，注解，观念词，习题和面试题，代码实战，以及如何高效学习的指导。
```

## 4.2 结合 prompt 提问

对于 LLM 来说，prompt 可以让更好的发挥大模型的能力。

我们首先定义了一个提示模板。它包含一些关于如何使用下面的上下文片段的说明，然后有一个上下文变量的占位符。

```python
from langchain.prompts import PromptTemplate

# Build prompt
template = """使用以下上下文片段来回答最后的问题。如果你不知道答案，只需说不知道，不要试图编造答案。答案最多使用三个句子。尽量简明扼要地回答。在回答的最后一定要说"感谢您的提问！"
{context}
问题：{question}
有用的回答："""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
```

```python
# Run chain
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)
```

```python
question = " 2025 年大语言模型效果最好的是哪个模型"
```

```python
result = qa_chain({"query": question})
print(f"LLM 对问题的回答：{result['result']}")
```

```markup
LLM 对问题的回答：2025年，大语言模型的效果将取决于各种技术的发展情况。目前，OpenAI 的 GPT-3 模型是最先进的大语言模型，它的效果非常出色。但是，随着技术的发展，2025 年可能会有更先进的模型出现，效果更好。感谢您的提问！
```

这里因为没有对应的信息，所以大语言模型只能回答不知道。您可以将知识库的内容调整为大语言模型综述的内容重新进行尝试。

```python
print(f"向量数据库检索到的最相关的文档：{result['source_documents'][0]}")
```

```markup
向量数据库检索到的最相关的文档：page_content='在本模块，我们将与读者分享提升大语言模型应用效果的各种技巧和最佳实践。书中内容涵盖广泛，包括软件开发提示词设计、文本总结、推理、转换、扩展以及构建聊天机器人等语言模型典型应用场景。我们衷心希望该课程能激发读者的想象力，开发出更出色的语言模型应用。\n\n随着 LLM 的发展，其大致可以分为两种类型，后续称为基础 LLM 和指令微调（Instruction Tuned）LLM。基础LLM是基于文本训练数据，训练出预测下一个单词能力的模型。其通常通过在互联网和其他来源的大量数据上训练，来确定紧接着出现的最可能的词。例如，如果你以“从前，有一只独角兽”作为 Prompt ，基础 LLM 可能会继续预测“她与独角兽朋友共同生活在一片神奇森林中”。但是，如果你以“法国的首都是什么”为 Prompt ，则基础 LLM 可能会根据互联网上的文章，将回答预测为“法国最大的城市是什么？法国的人口是多少？”，因为互联网上的文章很可能是有关法国国家的问答题目列表。' metadata={'source': '../knowledge_base/prompt_engineering/1. 简介 Introduction.md'}
```

这种方法非常好，因为它只涉及对语言模型的一次调用。然而，它也有局限性，即如果文档太多，可能无法将它们全部适配到上下文窗口中。

langchain 提供了几种不同的处理文档的方法：

| 类型        | 定义/区别                                                    | 优点                                                         | 缺点                                                         |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Stuff       | 将整个文本内容一次性输入给大模型进行处理。                   | - 只调用大模型一次，节省计算资源和时间。- 上下文信息完整，有助于理解整体语义。- 适用于处理较短的文本内容。 | - 不适用于处理较长的文本内容，可能导致模型过载。             |
| Refine      | 通过多次调用大模型逐步改进文本质量，进行多次迭代优化。       | - 可以在多次迭代中逐步改进文本质量。- 适用于需要进行多次迭代优化的场景。 | - 增加了计算资源和时间的消耗。- 可能需要多轮迭代才能达到期望的文本质量。- 不适用于实时性要求较高的场景。 |
| Map reduce  | 将大模型应用于每个文档，并将输出作为新文档传递给另一个模型，最终得到单个输出。 | - 可以对多个文档进行并行处理，提高处理效率。- 可以通过多次迭代处理实现优化。- 适用于需要对多个文档进行处理和合并的场景。 | - 增加了计算资源和时间的消耗。- 可能需要多轮迭代才能达到期望的结果。- 不适用于处理单个文档的场景。 |
| Map re-rank | 在每个文档上运行初始提示，为答案给出一个分数，返回得分最高的响应。 | - 可以根据置信度对文档进行排序和选择，提高结果的准确性。- 可以提供更可靠的答案。- 适用于需要根据置信度对文档进行排序和选择的场景。 | - 增加了计算资源和时间的消耗。- 可能需要对多个文档进行评分和排序。- 不适用于不需要对文档进行排序和选择的场景。 |

我们可以根据需要配置 chain_type 的参数，选择对应的处理方式。如：

```markup
RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    chain_type="map_reduce"
)
```























